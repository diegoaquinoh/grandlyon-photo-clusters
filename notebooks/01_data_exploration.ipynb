{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Exploration & Profiling\n",
    "\n",
    "**Objective:** Load the Flickr dataset, understand its schema, and identify data quality issues.\n",
    "\n",
    "## Schema\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| id | Photo ID |\n",
    "| user | Flickr user ID |\n",
    "| lat, long | GPS coordinates |\n",
    "| tags | Comma-separated tags |\n",
    "| title | Photo title |\n",
    "| date_taken_* | When photo was taken (minute, hour, day, month, year) |\n",
    "| date_upload_* | When photo was uploaded |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "DATA_PATH = '../data/flickr_data2.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "missing = pd.DataFrame({\n",
    "    'missing_count': df.isnull().sum(),\n",
    "    'missing_pct': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "missing = missing[missing['missing_count'] > 0].sort_values('missing_pct', ascending=False)\n",
    "\n",
    "print(\"=== Missing Values ===\")\n",
    "if len(missing) > 0:\n",
    "    print(missing)\n",
    "else:\n",
    "    print(\"No missing values found!\")\n",
    "\n",
    "# Visualize\n",
    "if len(missing) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    missing['missing_pct'].plot(kind='barh', ax=ax, color='coral')\n",
    "    ax.set_xlabel('Missing %')\n",
    "    ax.set_title('Missing Values by Column')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Duplicates Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate photo IDs\n",
    "dup_id = df['id'].duplicated().sum()\n",
    "print(f\"Duplicate photo IDs: {dup_id:,} ({dup_id/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Check for exact duplicate rows\n",
    "dup_rows = df.duplicated().sum()\n",
    "print(f\"Exact duplicate rows: {dup_rows:,} ({dup_rows/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Check for duplicate coordinates (same user, same location)\n",
    "dup_coords = df.duplicated(subset=['user', 'lat', 'long']).sum()\n",
    "print(f\"Duplicate (user + coordinates): {dup_coords:,} ({dup_coords/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some duplicates if they exist\n",
    "if dup_id > 0:\n",
    "    dup_ids = df[df['id'].duplicated(keep=False)]['id'].unique()[:5]\n",
    "    print(\"Sample duplicate photo IDs:\")\n",
    "    display(df[df['id'].isin(dup_ids)].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPS Coordinates Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPS statistics\n",
    "print(\"=== GPS Coordinate Statistics ===\")\n",
    "print(f\"\\nLatitude range:  [{df['lat'].min():.6f}, {df['lat'].max():.6f}]\")\n",
    "print(f\"Longitude range: [{df['long'].min():.6f}, {df['long'].max():.6f}]\")\n",
    "\n",
    "# Lyon approximate bounding box\n",
    "LYON_LAT_MIN, LYON_LAT_MAX = 45.55, 45.95\n",
    "LYON_LON_MIN, LYON_LON_MAX = 4.65, 5.10\n",
    "\n",
    "# Check for coordinates outside Lyon area\n",
    "outside_lyon = df[\n",
    "    (df['lat'] < LYON_LAT_MIN) | (df['lat'] > LYON_LAT_MAX) |\n",
    "    (df['long'] < LYON_LON_MIN) | (df['long'] > LYON_LON_MAX)\n",
    "]\n",
    "print(f\"\\nPoints outside Lyon area: {len(outside_lyon):,} ({len(outside_lyon)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Check for null/invalid coordinates\n",
    "null_coords = df[df['lat'].isnull() | df['long'].isnull()]\n",
    "print(f\"Null coordinates: {len(null_coords):,}\")\n",
    "\n",
    "# Check for (0, 0) coordinates\n",
    "zero_coords = df[(df['lat'] == 0) & (df['long'] == 0)]\n",
    "print(f\"Zero coordinates (0, 0): {len(zero_coords):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coordinate distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of latitudes\n",
    "axes[0].hist(df['lat'], bins=100, color='steelblue', alpha=0.7)\n",
    "axes[0].axvline(LYON_LAT_MIN, color='red', linestyle='--', label=f'Lyon min ({LYON_LAT_MIN})')\n",
    "axes[0].axvline(LYON_LAT_MAX, color='red', linestyle='--', label=f'Lyon max ({LYON_LAT_MAX})')\n",
    "axes[0].set_xlabel('Latitude')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Latitude Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Histogram of longitudes\n",
    "axes[1].hist(df['long'], bins=100, color='darkorange', alpha=0.7)\n",
    "axes[1].axvline(LYON_LON_MIN, color='red', linestyle='--', label=f'Lyon min ({LYON_LON_MIN})')\n",
    "axes[1].axvline(LYON_LON_MAX, color='red', linestyle='--', label=f'Lyon max ({LYON_LON_MAX})')\n",
    "axes[1].set_xlabel('Longitude')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Longitude Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of coordinates\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Sample for performance\n",
    "sample = df.sample(min(50000, len(df)), random_state=42)\n",
    "ax.scatter(sample['long'], sample['lat'], alpha=0.1, s=1, c='blue')\n",
    "\n",
    "# Draw Lyon bounding box\n",
    "ax.axhline(LYON_LAT_MIN, color='red', linestyle='--', alpha=0.5)\n",
    "ax.axhline(LYON_LAT_MAX, color='red', linestyle='--', alpha=0.5)\n",
    "ax.axvline(LYON_LON_MIN, color='red', linestyle='--', alpha=0.5)\n",
    "ax.axvline(LYON_LON_MAX, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('Photo Locations (50K sample)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Date Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct datetime from components\n",
    "date_cols = ['date_taken_year', 'date_taken_month', 'date_taken_day', \n",
    "             'date_taken_hour', 'date_taken_minute']\n",
    "\n",
    "print(\"=== Date Component Statistics ===\")\n",
    "for col in date_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: min={df[col].min()}, max={df[col].max()}, null={df[col].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datetime column\n",
    "def create_datetime(row):\n",
    "    try:\n",
    "        return datetime(\n",
    "            int(row['date_taken_year']),\n",
    "            int(row['date_taken_month']),\n",
    "            int(row['date_taken_day']),\n",
    "            int(row['date_taken_hour']),\n",
    "            int(row['date_taken_minute'])\n",
    "        )\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply to sample first to check for issues\n",
    "sample_dates = df.head(1000).apply(create_datetime, axis=1)\n",
    "invalid_dates = sample_dates.isnull().sum()\n",
    "print(f\"Invalid dates in first 1000 rows: {invalid_dates}\")\n",
    "\n",
    "# Check for unrealistic date ranges\n",
    "print(f\"\\nYear range: {df['date_taken_year'].min()} - {df['date_taken_year'].max()}\")\n",
    "print(f\"Future dates (year > 2026): {(df['date_taken_year'] > 2026).sum()}\")\n",
    "print(f\"Very old dates (year < 2000): {(df['date_taken_year'] < 2000).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "df['date_taken_year'].value_counts().sort_index().plot(kind='bar', ax=ax, color='teal')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Number of Photos')\n",
    "ax.set_title('Photos by Year Taken')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "month_counts = df['date_taken_month'].value_counts().sort_index()\n",
    "month_counts.plot(kind='bar', ax=ax, color='purple')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Number of Photos')\n",
    "ax.set_title('Photos by Month (Seasonality)')\n",
    "ax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Analysis (Tags & Titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags analysis\n",
    "print(\"=== Tags Analysis ===\")\n",
    "empty_tags = df['tags'].isnull() | (df['tags'] == '')\n",
    "print(f\"Photos without tags: {empty_tags.sum():,} ({empty_tags.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Count tags per photo\n",
    "def count_tags(tag_str):\n",
    "    if pd.isna(tag_str) or tag_str == '':\n",
    "        return 0\n",
    "    return len(str(tag_str).split(','))\n",
    "\n",
    "df['tag_count'] = df['tags'].apply(count_tags)\n",
    "print(f\"\\nTags per photo: mean={df['tag_count'].mean():.1f}, median={df['tag_count'].median():.0f}, max={df['tag_count'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common tags\n",
    "all_tags = []\n",
    "for tags in df['tags'].dropna():\n",
    "    all_tags.extend([t.strip().lower() for t in str(tags).split(',') if t.strip()])\n",
    "\n",
    "from collections import Counter\n",
    "tag_counts = Counter(all_tags)\n",
    "print(\"\\n=== Top 30 Tags ===\")\n",
    "for tag, count in tag_counts.most_common(30):\n",
    "    print(f\"{tag}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title analysis\n",
    "print(\"\\n=== Title Analysis ===\")\n",
    "empty_titles = df['title'].isnull() | (df['title'] == '')\n",
    "print(f\"Photos without title: {empty_titles.sum():,} ({empty_titles.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Title length\n",
    "df['title_len'] = df['title'].fillna('').apply(len)\n",
    "print(f\"Title length: mean={df['title_len'].mean():.1f}, median={df['title_len'].median():.0f}, max={df['title_len'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample titles\n",
    "print(\"\\n=== Sample Titles ===\")\n",
    "sample_titles = df[df['title'].notna() & (df['title'] != '')]['title'].sample(20, random_state=42)\n",
    "for i, title in enumerate(sample_titles, 1):\n",
    "    print(f\"{i}. {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. User Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User statistics\n",
    "print(\"=== User Statistics ===\")\n",
    "print(f\"Unique users: {df['user'].nunique():,}\")\n",
    "\n",
    "photos_per_user = df['user'].value_counts()\n",
    "print(f\"\\nPhotos per user: mean={photos_per_user.mean():.1f}, median={photos_per_user.median():.0f}\")\n",
    "print(f\"Max photos by single user: {photos_per_user.max():,}\")\n",
    "print(f\"Users with only 1 photo: {(photos_per_user == 1).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top users\n",
    "print(\"\\n=== Top 10 Users by Photo Count ===\")\n",
    "print(photos_per_user.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary of Issues Found\n",
    "\n",
    "Run this cell to generate a summary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATA QUALITY ISSUES SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "issues = []\n",
    "\n",
    "# Duplicates\n",
    "if dup_id > 0:\n",
    "    issues.append(f\"⚠️ Duplicate photo IDs: {dup_id:,}\")\n",
    "if dup_rows > 0:\n",
    "    issues.append(f\"⚠️ Exact duplicate rows: {dup_rows:,}\")\n",
    "\n",
    "# GPS\n",
    "if len(null_coords) > 0:\n",
    "    issues.append(f\"⚠️ Null coordinates: {len(null_coords):,}\")\n",
    "if len(zero_coords) > 0:\n",
    "    issues.append(f\"⚠️ Zero (0,0) coordinates: {len(zero_coords):,}\")\n",
    "if len(outside_lyon) > 0:\n",
    "    issues.append(f\"⚠️ Points outside Lyon bbox: {len(outside_lyon):,}\")\n",
    "\n",
    "# Dates\n",
    "future = (df['date_taken_year'] > 2026).sum()\n",
    "old = (df['date_taken_year'] < 2000).sum()\n",
    "if future > 0:\n",
    "    issues.append(f\"⚠️ Future dates (>2026): {future:,}\")\n",
    "if old > 0:\n",
    "    issues.append(f\"⚠️ Very old dates (<2000): {old:,}\")\n",
    "\n",
    "# Text\n",
    "if empty_tags.sum() > len(df) * 0.1:\n",
    "    issues.append(f\"ℹ️ Photos without tags: {empty_tags.sum():,} ({empty_tags.sum()/len(df)*100:.1f}%)\")\n",
    "if empty_titles.sum() > len(df) * 0.1:\n",
    "    issues.append(f\"ℹ️ Photos without title: {empty_titles.sum():,} ({empty_titles.sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "if issues:\n",
    "    for issue in issues:\n",
    "        print(issue)\n",
    "else:\n",
    "    print(\"✅ No major issues found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS: Data Cleaning\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Remove duplicate rows/IDs\")\n",
    "print(\"2. Filter invalid GPS coordinates\")\n",
    "print(\"3. Handle date parsing issues\")\n",
    "print(\"4. Create cleaned Parquet file for efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Save Profiling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate results\n",
    "profiling_stats = {\n",
    "    'total_rows': len(df),\n",
    "    'unique_users': df['user'].nunique(),\n",
    "    'duplicate_ids': dup_id,\n",
    "    'duplicate_rows': dup_rows,\n",
    "    'null_coords': len(null_coords),\n",
    "    'outside_lyon': len(outside_lyon),\n",
    "    'empty_tags': empty_tags.sum(),\n",
    "    'empty_titles': empty_titles.sum(),\n",
    "    'year_range': (int(df['date_taken_year'].min()), int(df['date_taken_year'].max())),\n",
    "    'lat_range': (df['lat'].min(), df['lat'].max()),\n",
    "    'lon_range': (df['long'].min(), df['long'].max()),\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../reports/profiling_stats.json', 'w') as f:\n",
    "    json.dump(profiling_stats, f, indent=2, default=str)\n",
    "\n",
    "print(\"Profiling stats saved to reports/profiling_stats.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
